{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2021 Vaibhav Singh (@vaibhav016)\n",
    "# Copyright 2021 Dr Vinayak Abrol (_)\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signal-forward",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import math\n",
    "\n",
    "import cv2\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow_asr.gradient_visualisation.plotting_utils import make_directories\n",
    "from tensorflow_asr.utils import env_util\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moral-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_util.setup_environment()\n",
    "\n",
    "DEFAULT_YAML = \"/Users/vaibhavsingh/Desktop/FILRCN/contextnet/config.yml\"\n",
    "\n",
    "directory_to_save_gradient_lists = make_directories(os.getcwd(), \"gradient_lists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_asr.configs.config import Config\n",
    "from tensorflow_asr.datasets.asr_dataset import ASRSliceDataset\n",
    "from tensorflow_asr.featurizers.speech_featurizers import TFSpeechFeaturizer\n",
    "from tensorflow_asr.featurizers.text_featurizers import CharFeaturizer\n",
    "from tensorflow_asr.models.transducer.contextnet import ContextNet\n",
    "from tensorflow_asr.optimizers.schedules import TransformerSchedule\n",
    "from tensorflow_asr.utils import env_util\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "detailed-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": False})\n",
    "strategy = env_util.setup_strategy([0])\n",
    "config = Config(DEFAULT_YAML)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-caution",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = \"/content/FILRCN/examples/checkpoints\"\n",
    "last_trained_model = os.path.join(model_directory, sorted(os.listdir(model_directory))[-1])\n",
    "\n",
    "speech_featurizer = TFSpeechFeaturizer(config.speech_config)\n",
    "\n",
    "text_featurizer = CharFeaturizer(config.decoder_config)\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "visualisation_dataset = ASRSliceDataset(\n",
    "    speech_featurizer=speech_featurizer,\n",
    "    text_featurizer=text_featurizer,\n",
    "    **vars(config.learning_config.gradient_dataset_vis_config)\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "visualisation_gradient_loader = visualisation_dataset.create(batch_size)\n",
    "contextnet = ContextNet(**config.model_config, vocabulary_size=text_featurizer.num_classes)\n",
    "contextnet.make(speech_featurizer.shape)\n",
    "contextnet.load_weights(last_trained_model, by_name=True)\n",
    "contextnet.add_featurizers(speech_featurizer, text_featurizer)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    TransformerSchedule(\n",
    "        d_model=contextnet.dmodel,\n",
    "        warmup_steps=config.learning_config.optimizer_config.pop(\"warmup_steps\", 10000),\n",
    "        max_lr=(0.05 / math.sqrt(contextnet.dmodel))\n",
    "    ),\n",
    "    **config.learning_config.optimizer_config\n",
    ")\n",
    "\n",
    "contextnet.compile(\n",
    "    optimizer=optimizer,\n",
    "    steps_per_execution=1,\n",
    "    global_batch_size=1,\n",
    "    blank=text_featurizer.blank\n",
    ")\n",
    "encoder = contextnet.layers[0]\n",
    "\n",
    "activated_node_list = []\n",
    "random_activated_node_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the last node\n",
    "\n",
    "for i, j in visualisation_gradient_loader:\n",
    "    inputs = tf.Variable(i[\"inputs\"])\n",
    "    inputs_length = tf.Variable(i[\"inputs_length\"])\n",
    "    signal = tf.Variable(i[\"signal\"])\n",
    "\n",
    "    encoder_output = encoder.call_feature_output([inputs, inputs_length, signal])\n",
    "    activated_channels = tf.norm(encoder_output, axis=1)\n",
    "    activated_node_index = tf.math.argmax(activated_channels, axis=1).numpy()\n",
    "\n",
    "    activated_node_list.append(activated_node_index[0])\n",
    "    random_activated_node_list.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-criterion",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_integrated_gradients(encoder, mel_spec, inputs_length, signal, activated_node_index, random_node_index):\n",
    "    m_steps = 50\n",
    "    baseline = tf.zeros(shape=mel_spec.shape)\n",
    "    alphas = tf.linspace(start=0.0, stop=1.0, num=m_steps + 1)\n",
    "    print(\"alphas\", alphas.shape)\n",
    "    alphas_x = alphas[:, tf.newaxis, tf.newaxis]\n",
    "    print(\"alphas_x\", alphas_x.shape)\n",
    "    baseline_x = tf.expand_dims(baseline, axis=0)\n",
    "    print(\"baseline \", baseline_x.shape)\n",
    "    input_x = tf.expand_dims(mel_spec, axis=0)\n",
    "    print(\"input\", input_x.shape)\n",
    "    delta = input_x - baseline_x\n",
    "    interpolated_images = baseline_x + alphas_x * delta\n",
    "    print(\"final images\", interpolated_images.shape)\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(interpolated_images)\n",
    "        images = tf.expand_dims(interpolated_images, axis=-1)\n",
    "        print(images.shape)\n",
    "        encoder_output = encoder.call_feature_output([images, inputs_length, signal])\n",
    "        gradients = tape.gradient(encoder_output[:, :, activated_node_index], interpolated_images)\n",
    "\n",
    "        random_gradients = tape.gradient(encoder_output[:, :, random_node_index], interpolated_images)\n",
    "\n",
    "    grads = (gradients[:-1] + gradients[1:]) / tf.constant(2.0)\n",
    "    random_grads = (random_gradients[:-1] + random_gradients[1:]) / tf.constant(2.0)\n",
    "\n",
    "    integrated_gradients = tf.math.reduce_mean(grads, axis=0)\n",
    "    integrated_random_gradients = tf.math.reduce_mean(random_grads, axis=0)\n",
    "\n",
    "    return integrated_gradients, integrated_random_gradients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in tqdm(sorted(os.listdir(model_directory))):\n",
    "    if not filename.endswith(\".h5\"):\n",
    "        print(filename)\n",
    "        continue\n",
    "\n",
    "    gradient_file = filename.split('.')[0]\n",
    "    model_name = os.path.join(model_directory, filename)\n",
    "    print(\"model being processed now: \", model_name)\n",
    "\n",
    "    contextnet.load_weights(model_name, by_name=True)\n",
    "    encoder = contextnet.layers[0]\n",
    "\n",
    "    m = 0\n",
    "    images_check = []\n",
    "    gradients_check = []\n",
    "    random_gradients_check = []\n",
    "    for i, j in visualisation_gradient_loader:\n",
    "        inputs = tf.Variable(i[\"inputs\"])\n",
    "        inputs_length = tf.Variable(i[\"inputs_length\"])\n",
    "        signal = tf.Variable(i[\"signal\"])\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            tape.watch(inputs)\n",
    "            encoder_output = encoder.call_feature_output([inputs, inputs_length, signal])\n",
    "            gradients = tape.gradient(encoder_output[:, :, activated_node_list[m]], inputs)\n",
    "            random_gradients = tape.gradient(encoder_output[:, :, random_activated_node_list[m]], inputs)\n",
    "\n",
    "        interated_gradients, random_integrated_gradients = get_integrated_gradients(encoder, tf.squeeze(inputs),\n",
    "                                                                                    inputs_length, signal,\n",
    "                                                                                    activated_node_list[m],\n",
    "                                                                                    random_activated_node_list[m])\n",
    "\n",
    "        gradients_check.append(interated_gradients)\n",
    "        random_gradients_check.append(random_integrated_gradients)\n",
    "\n",
    "        images_check.append(tf.squeeze(inputs))\n",
    "\n",
    "        print(\"integrated_gradients shape=========\", interated_gradients.shape, random_integrated_gradients.shape)\n",
    "        m = m + 1\n",
    "\n",
    "    dd = {'input_image': images_check,\n",
    "          'integrated_gradients': gradients_check,\n",
    "          'random_integrated_gradients': random_gradients_check,\n",
    "          'index_of_activated_node': activated_node_list,\n",
    "          'index_of_random_node': random_activated_node_list\n",
    "          }\n",
    "\n",
    "    file_path_to_save = os.path.join(directory_to_save_gradient_lists, filename)\n",
    "\n",
    "    with open(file_path_to_save + \".pkl\", 'wb') as f:\n",
    "        pickle.dump(dd, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "common-attempt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_cmap(color_map):\n",
    "    return plt.get_cmap(color_map)\n",
    "\n",
    "\n",
    "def make_directories(current_working_directory_abs, directory_name):\n",
    "    _directory_abs = os.path.join(current_working_directory_abs, directory_name)\n",
    "    try:\n",
    "        os.mkdir(_directory_abs)\n",
    "    except Exception as e:\n",
    "        print(\"--------------\", directory_name, \"directory already exists-----------------\")\n",
    "        print(\"--------------The contents will be over-ridden-------------------\")\n",
    "        return _directory_abs\n",
    "\n",
    "    return _directory_abs\n",
    "\n",
    "\n",
    "def normalize_gradients(gradient_directory):\n",
    "    norm_max_g = -1\n",
    "    normm_max_r = -1\n",
    "\n",
    "    for index, file in enumerate(sorted(os.listdir(gradient_directory))):\n",
    "        filename = os.path.join(gradient_directory, file)\n",
    "        print(\"filenmes \", filename)\n",
    "        with open(filename, \"rb\") as f:\n",
    "            x_temp = pickle.load(f)\n",
    "\n",
    "        gradients_check = x_temp[\"integrated_gradients\"]\n",
    "        random_gradients = x_temp[\"random_integrated_gradients\"]\n",
    "\n",
    "        for i, j in zip(gradients_check, random_gradients):\n",
    "            norm_max_g = tf.maximum(tf.norm(i), norm_max_g)\n",
    "            normm_max_r = tf.maximum(tf.norm(i), normm_max_r)\n",
    "\n",
    "            print(\"normm=======\", norm_max_g, normm_max_r)\n",
    "\n",
    "    return norm_max_g, normm_max_r\n",
    "\n",
    "cmap = \"jet\"\n",
    "index_fixed = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-convertible",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_working_directory = os.getcwd()\n",
    "# compute gradient lists \n",
    "gradient_directory = os.path.join(current_working_directory, \"gradient_lists\")\n",
    "directory_to_save_plots = make_directories(current_working_directory, \"gradient_plots\")\n",
    "directory_to_save_video = make_directories(current_working_directory, \"video\")\n",
    "norm_g, norm_r = normalize_gradients(gradient_directory)\n",
    "\n",
    "def gradient_transformation(gradient, norm):\n",
    "    # gradient = tf.abs(gradient)\n",
    "    # gradient = tf.square(gradient)\n",
    "    # gradient = gradient/norm\n",
    "    # print(gradient)\n",
    "\n",
    "    return gradient.numpy().T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradients_images(directory_to_save_plots):\n",
    "    for index, file in enumerate(sorted(os.listdir(gradient_directory))):\n",
    "        filename = os.path.join(gradient_directory, file)\n",
    "        print(\"gradient list file =>  \",filename)\n",
    "        with open(filename, \"rb\") as f:\n",
    "            x_temp = pickle.load(f)\n",
    "\n",
    "        name = file.split('.')[0].split('_')[-1]\n",
    "\n",
    "        images_check = x_temp['input_image']\n",
    "        gradients_check = x_temp[\"integrated_gradients\"]\n",
    "        random_gradients = x_temp[\"random_integrated_gradients\"]\n",
    "        activated_node_list = x_temp[\"index_of_activated_node\"]\n",
    "        random_node_list = x_temp[\"index_of_random_node\"]\n",
    "\n",
    "        fig, ax = plt.subplots(nrows=2, ncols=3, figsize=(25, 14), facecolor=(1, 1, 1))\n",
    "        fig.suptitle(' Model checkpoint ' + str(index), fontsize=16)\n",
    "        for i, j in enumerate(zip(images_check, gradients_check)):\n",
    "            # this is to obtain other samples from visualisation data(it has 6 files- even=>male, and odd=>female\n",
    "            # if i<4:\n",
    "            #     continue\n",
    "            print(images_check[i+1].shape)\n",
    "            print(gradients_check[i].shape)\n",
    "\n",
    "            ax[0][0].set(title=\" Log Mel spectrogram for male voice\")\n",
    "            img = librosa.display.specshow(images_check[i].numpy().T, sr=16000,\n",
    "                                           fmax=8000, x_axis='time', y_axis='mel', ax=ax[0][0], alpha=1)\n",
    "            plt.colorbar(img, ax=ax[0][0])\n",
    "            ax[0][0].label_outer()\n",
    "\n",
    "            title = \" Gradient Attribution for filter \" + str(activated_node_list[i])\n",
    "            ax[0][1].set(title=title)\n",
    "            img2 = librosa.display.specshow(gradient_transformation(gradients_check[i], norm_g), sr=16000,\n",
    "                                            fmax=8000, x_axis='time', y_axis='mel',cmap=obtain_cmap(cmap), ax=ax[0][1],  alpha=1)\n",
    "            if index_fixed:\n",
    "                img2.set_clim(vmin=-0.1, vmax=0.1)\n",
    "            plt.colorbar(img2, ax=ax[0][1])\n",
    "            ax[0][1].label_outer()\n",
    "\n",
    "            title = \" Gradient Attribution for filter \" + str(random_node_list[i])\n",
    "            ax[0][2].set(title=title)\n",
    "            img2 = librosa.display.specshow(gradient_transformation(random_gradients[i], norm_r), sr=16000,\n",
    "                                            fmax=8000, x_axis='time', y_axis='mel', cmap=obtain_cmap(cmap),ax=ax[0][2], alpha=1)\n",
    "            if index_fixed:\n",
    "                img2.set_clim(vmin=-0.1, vmax=0.1)\n",
    "            plt.colorbar(img2, ax=ax[0][2])\n",
    "            ax[0][2].label_outer()\n",
    "\n",
    "    ############################################################  2nd row of female#######################################################\n",
    "\n",
    "            ax[1][0].set(title=\" Log Mel spectrogram for female voice\")\n",
    "            img = librosa.display.specshow(images_check[i + 1].numpy().T, sr=16000,\n",
    "                                           fmax=8000, x_axis='time', y_axis='mel', ax=ax[1][0], alpha=1)\n",
    "            plt.colorbar(img, ax=ax[1][0])\n",
    "            ax[1][0].label_outer()\n",
    "\n",
    "\n",
    "\n",
    "            img2 = librosa.display.specshow(gradient_transformation(gradients_check[i+1], norm_g), sr=16000,\n",
    "                                            fmax=8000, x_axis='time', y_axis='mel', cmap=obtain_cmap(cmap), ax=ax[1][1],  alpha=1)\n",
    "            if index_fixed:\n",
    "                img2.set_clim(vmin=-0.1, vmax=0.1)\n",
    "            plt.colorbar(img2, ax=ax[1][1])\n",
    "            ax[1][1].label_outer()\n",
    "\n",
    "            img2 = librosa.display.specshow(gradient_transformation(random_gradients[i+1], norm_r), sr=16000,\n",
    "                                            fmax=8000, x_axis='time', y_axis='mel', cmap=obtain_cmap(cmap), ax=ax[1][2], alpha=1)\n",
    "            if index_fixed:\n",
    "                img2.set_clim(vmin=-0.1, vmax=0.1)\n",
    "            plt.colorbar(img2, ax=ax[1][2])\n",
    "            ax[1][2].label_outer()\n",
    "\n",
    "\n",
    "            # this break means that only 2 images(male and female) will be displayed in a plot.\n",
    "            # This is not a sanity break. Its purposeful.\n",
    "            break\n",
    "\n",
    "        plt.savefig(directory_to_save_plots + \"/Grad\" + name +\".png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-couple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_videos_from_images(directory_to_save_video):\n",
    "    img_array = []\n",
    "    size = (10, 10)\n",
    "\n",
    "\n",
    "\n",
    "    fname1 = os.path.join(os.getcwd(), \"gradient_plots\") + \"/*.png\"\n",
    "    # fname1 = '/Users/vaibhavsingh/Desktop/TensorFlowASR/examples/contextnet/contextnet_visualisation/gradient_visualisation/grad_vis_4/*.png'\n",
    "\n",
    "    for filename1 in (sorted(glob.glob(fname1))):\n",
    "        print(filename1)\n",
    "        image1 = cv2.imread(filename1)\n",
    "        height, width, layers = image1.shape\n",
    "        size = (width, height)\n",
    "        print(size)\n",
    "        img_array.append(image1)\n",
    "\n",
    "    out = cv2.VideoWriter(directory_to_save_video + \"/gradient_vis.avi\", cv2.VideoWriter_fourcc(*'DIVX'), 1, size)\n",
    "\n",
    "    for i in range(len(img_array)):\n",
    "        out.write(img_array[i])\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-throw",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradients_images(directory_to_save_plots)\n",
    "make_videos_from_images(directory_to_save_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
